{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Loading the Data\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import json\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"/home/guilherme/Documents/GitHub/Tese/MDCompass/\")\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from ML_Algorithm.Functions.compute_symptom_relatedness import compute_relatedness, compute_relatedness_matrix\n",
    "\n",
    "# Loading the severity scores\n",
    "with open(\"/home/guilherme/Documents/GitHub/Tese/MDCompass/ML_Algorithm/json_files/severity_scores.json\", \"r\") as f:\n",
    "    severity_mapping = json.load(f)\n",
    "\n",
    "file_path = \"/home/guilherme/Documents/GitHub/Tese/Documentation/Dataset_Augmentation/Raw_Augmented_database_13_10_2023.xlsx\"\n",
    "df_raw = pd.read_excel(file_path, sheet_name=\"Original Data\")\n",
    "\n",
    "file_path = \"/home/guilherme/Documents/GitHub/Tese/Documentation/Dataset_Augmentation/Augmented_database_29_09_2023.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Original Data\")\n",
    "\n",
    "file_path = \"/home/guilherme/Documents/GitHub/Tese/Documentation/Dataset_Augmentation/Augmented_database_29_09_2023.xlsx\"\n",
    "df_augmented = pd.read_excel(file_path, sheet_name=\"Augmented Data\")\n",
    "\n",
    "## Loading the models\n",
    "NoFeature_label_encoder = \"loaded_models/no_features_label_encoder.pkl\"\n",
    "WithFeature_label_encoder = \"loaded_models/with_features_label_encoder.pkl\"\n",
    "Embeddings_feature_label_encoder = \"loaded_models/raw_X_padded.pkl\"\n",
    "BothWithFeature_label_encoder = \"loaded_models/both_features_label_encoder.pkl\"\n",
    "\n",
    "\n",
    "NoFeature_symptom_binarizer = \"loaded_models/no_features_symptoms_binarizer.pkl\"\n",
    "WithFeature_symptom_binarizer = \"loaded_models/with_features_symptoms_binarizer.pkl\"\n",
    "BothWithFeature_symptom_binarizer = \"loaded_models/both_features_symptoms_binarizer.pkl\"\n",
    "\n",
    "KNN_NoFeature = \"loaded_models/KNN_no_features.pkl\"\n",
    "KNN_WithFeature = \"loaded_models/KNN_with_features.pkl\"\n",
    "RTF_NoFeature = \"loaded_models/RTF_no_features.pkl\"\n",
    "RTF_WithFeature = \"loaded_models/RTF_with_features.pkl\"\n",
    "NN_NoFeature = \"loaded_models/NN_no_features.pkl\"\n",
    "NN_WithFeature = \"loaded_models/NN_with_features.pkl\"\n",
    "\n",
    "NN_BothFeatures = \"loaded_models/NN_with_features_BOTH.pkl\"\n",
    "\n",
    "Embedding_model = \"loaded_models/Raw_Embedded_model.pkl\"\n",
    "w2v_model = \"loaded_models/raw_w2v_model.pkl\"\n",
    "\n",
    "\n",
    "# Load models and other necessary components\n",
    "# model_paths = [KNN_NoFeature, RTF_NoFeature, NN_NoFeature] # Add paths to your models\n",
    "# model_paths = [KNN_WithFeature, RTF_WithFeature, NN_WithFeature] # Add paths to your models\n",
    "model_paths = [NN_NoFeature, NN_WithFeature, Embedding_model] # Add paths to your models\n",
    "\n",
    "# \n",
    "binarizer_paths = [NoFeature_symptom_binarizer, WithFeature_symptom_binarizer, w2v_model, BothWithFeature_symptom_binarizer] # Corresponding binarizers\n",
    "label_encoder_paths = [NoFeature_label_encoder, WithFeature_label_encoder, Embeddings_feature_label_encoder, BothWithFeature_label_encoder] # Corresponding label encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extracting the sample data to feed the function and data initialization\n",
    "\n",
    "\n",
    "# Filter diseases with at least 10 symptoms\n",
    "filtered_data = df[df.notnull().sum(axis=1) >= 11]  # At least 11 non-null values (1 for ICD 11 + 10 symptoms)\n",
    "\n",
    "# Extract a sample of 10 diseases\n",
    "sample_data = filtered_data.sample(15)\n",
    "sample_diseases = sample_data[\"ICD 11\"].tolist()\n",
    "\n",
    "max_dim = 24 #Max number of symptoms\n",
    "\n",
    "# Extract 10 symptoms for each of these diseases\n",
    "sample_symptoms_list = []\n",
    "sample_symptoms_raw_list = []\n",
    "for _, row in sample_data.iterrows():\n",
    "    encoded_symptoms = row.dropna().values[3:13].tolist()\n",
    "    raw_symptoms = df_raw.loc[df['ICD 11'] == row['ICD 11']].dropna(axis=1).values[0][3:13].tolist()\n",
    "    sample_symptoms_list.append(encoded_symptoms)\n",
    "    sample_symptoms_raw_list.append(raw_symptoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Existing function for predicting diseases given a model\n",
    "\n",
    "# No feature function\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(df_augmented[\"ICD 11\"])\n",
    "embedding_size = 128\n",
    "\n",
    "\n",
    "def predict_disease(model, binarizer, label_encoder, symptoms_list, raw_symptoms_list, model_path, max_dim=max_dim, top_n=5):\n",
    "    \"\"\"\n",
    "    Given a list of symptoms (ICD-11 codes), predict the top potential diseases along with their confidence.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"Embedded\" in model_path:\n",
    "\n",
    "        embedded_symptoms = symptoms_to_embedding(raw_symptoms_list, binarizer, embedding_size)\n",
    "        embedded_symptoms = np.expand_dims(embedded_symptoms, 0)  # Add batch dimension\n",
    "\n",
    "        # Pad the embedded symptoms\n",
    "        features = pad_sequences(embedded_symptoms, maxlen=label_encoder.shape[1], padding='post')\n",
    "\n",
    "        disease_probabilities = model.predict(features)\n",
    "        \n",
    "        # Extract top predictions\n",
    "        top_indices = np.argsort(disease_probabilities[0])[-top_n:][::-1]\n",
    "        top_diseases = encoder.inverse_transform(top_indices)\n",
    "        top_probabilities = disease_probabilities[0][top_indices]\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        symptoms_encoded = binarizer.transform([symptoms_list])\n",
    "        features = symptoms_encoded\n",
    "\n",
    "        # Infer from the model's path if the model needs features\n",
    "        if \"with_features\" in model_path:\n",
    "            \n",
    "            if max_dim:\n",
    "                relatedness_values = compute_relatedness_matrix(symptoms_list)\n",
    "                flattened_relatedness = relatedness_values.reshape(-1)\n",
    "                padded_relatedness = pad_sequences([flattened_relatedness], maxlen=max_dim*max_dim, padding='post', dtype='float32').reshape(1, -1)\n",
    "                features = np.hstack((symptoms_encoded, padded_relatedness))\n",
    "\n",
    "        elif \"both_features\" in model_path:\n",
    "                severity_values = get_severity_scores(symptoms_list)\n",
    "                padded_severity = pad_sequences([severity_values], maxlen=max_dim, padding='post', dtype='float32').reshape(1, -1)\n",
    "\n",
    "                relatedness_values = compute_relatedness_matrix(symptoms_list)\n",
    "                flattened_relatedness = relatedness_values.reshape(-1)\n",
    "                padded_relatedness = pad_sequences([flattened_relatedness], maxlen=max_dim*max_dim, padding='post', dtype='float32').reshape(1, -1)\n",
    "\n",
    "                features = np.hstack((symptoms_encoded, padded_relatedness, padded_severity))\n",
    "\n",
    "\n",
    "\n",
    "        # Predict using the model\n",
    "        try:\n",
    "            # print(f\"features size: {features.shape}\")\n",
    "            disease_probabilities = model.predict_proba(features)\n",
    "        except AttributeError:\n",
    "            disease_probabilities = model.predict(features)\n",
    "        \n",
    "        # Extract top predictions\n",
    "        top_indices = np.argsort(disease_probabilities[0])[-top_n:][::-1]\n",
    "        top_diseases = label_encoder.inverse_transform(top_indices)\n",
    "        top_probabilities = disease_probabilities[0][top_indices]\n",
    "\n",
    "    return list(zip(top_diseases, top_probabilities))\n",
    "\n",
    "\n",
    "# Function to load a model, its binarizer, and label encoder from pickle files\n",
    "def load_model_from_files(model_path, binarizer_path, label_encoder_path):\n",
    "    model = joblib.load(model_path)\n",
    "    binarizer = joblib.load(binarizer_path)\n",
    "    label_encoder = joblib.load(label_encoder_path)\n",
    "    return model, binarizer, label_encoder\n",
    "\n",
    "def get_disease_name_from_code(code):\n",
    "    \"\"\"\n",
    "    Fetch the disease name corresponding to the given code from the dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return df[df['ICD 11'] == code]['Disease'].iloc[0]\n",
    "    except IndexError:\n",
    "        # Return code itself if not found\n",
    "        return code\n",
    "\n",
    "def symptoms_to_embedding(symptoms, model, embedding_size):\n",
    "    embedding_matrix = np.zeros((len(symptoms), embedding_size))\n",
    "    for i, symptom in enumerate(symptoms):\n",
    "        if symptom in model.wv:\n",
    "            embedding_matrix[i] = model.wv[str(symptom)]\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def get_severity_scores(symptom_list):\n",
    "    return [severity_mapping.get(symptom, 0) for symptom in symptom_list]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 0\n",
    "for i in range(len(model_paths)):\n",
    "    \n",
    "    if \"with_features\" in model_paths[i]:\n",
    "        val = 1\n",
    "    elif \"Embedded\" in model_paths[i]:\n",
    "        val = 2\n",
    "    elif \"both_features\" in model_paths[i]:\n",
    "        val = 3\n",
    "    else:\n",
    "        val = 0\n",
    "\n",
    "    model, binarizer, label_encoder = load_model_from_files(model_paths[i], binarizer_paths[val], label_encoder_paths[val])\n",
    "    print(f\"\\n\\nModel {i+1} Predictions:\")\n",
    "    for j, symptoms in enumerate(sample_symptoms_list):\n",
    "        raw_symptoms = sample_symptoms_raw_list[j]  # Get the corresponding raw symptoms\n",
    "        predicted_diseases_with_confidence = predict_disease(model, binarizer, label_encoder, symptoms, raw_symptoms, model_path=model_paths[i])\n",
    "        print(f\"\\nFor actual disease {sample_diseases[j]}:\")\n",
    "        for disease, confidence in predicted_diseases_with_confidence:\n",
    "            print(f\"Predicted Disease: {disease} with confidence: {confidence*100:.2f}%\")\n",
    "    print(\"==========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Lists to store values for visualization\n",
    "model_confidences = [[] for _ in model_paths]\n",
    "all_diseases = []\n",
    "\n",
    "for i in range(len(model_paths)):\n",
    "    \n",
    "    if \"with_features\" in model_paths[i]:\n",
    "        val = 1\n",
    "    elif \"Embedded\" in model_paths[i]:\n",
    "        val = 2\n",
    "    elif \"both_features\" in model_paths[i]:\n",
    "        val = 3\n",
    "    else:\n",
    "        val = 0\n",
    "\n",
    "    model, binarizer, label_encoder = load_model_from_files(model_paths[i], binarizer_paths[val], label_encoder_paths[val])\n",
    "    \n",
    "    for j, symptoms in enumerate(sample_symptoms_list):\n",
    "        raw_symptoms = sample_symptoms_raw_list[j]\n",
    "        predicted_diseases_with_confidence = predict_disease(model, binarizer, label_encoder, symptoms,raw_symptoms, model_path=model_paths[i])\n",
    "        \n",
    "        # If it's the first model, add the disease to all_diseases list\n",
    "        if i == 0:\n",
    "            all_diseases.append(sample_diseases[j])\n",
    "            \n",
    "        # Append the top confidence value for the current model\n",
    "        model_confidences[i].append(predicted_diseases_with_confidence[0][1])\n",
    "\n",
    "# Plotting\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(all_diseases))\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "for i, confidences in enumerate(model_confidences):\n",
    "    plt.bar(index + i*bar_width, [c*100 for c in confidences], bar_width, label=f'Model {i+1}', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Disease', fontsize=13)\n",
    "plt.ylabel('Top Prediction Confidence (%)', fontsize=13)\n",
    "plt.xticks(index + bar_width, all_diseases, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model_confidences to a 2D array suitable for a heatmap\n",
    "heatmap_data = np.array(model_confidences) * 100\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Prediction Confidence (%)'}, yticklabels=[f'Model {i+1}' for i in range(len(model_paths))], xticklabels=all_diseases, fmt=\".0f\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel('Disease', fontsize=13)\n",
    "plt.ylabel('Model', fontsize=13)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tabulated Predictions\n",
    "# ---------------------\n",
    "table_data = []\n",
    "\n",
    "# First, we create an empty list for each disease's row\n",
    "for _ in sample_symptoms_list:\n",
    "    table_data.append([])\n",
    "\n",
    "# Initialize a list to count correct predictions for each model\n",
    "correct_predictions = [0] * len(model_paths)\n",
    "\n",
    "# For each model, collect the top prediction and add it to the respective disease's row\n",
    "for i in range(len(model_paths)):\n",
    "\n",
    "    if \"with_features\" in model_paths[i]:\n",
    "        val = 1\n",
    "    elif \"Embedded\" in model_paths[i]:\n",
    "        val = 2\n",
    "    elif \"both_features\" in model_paths[i]:\n",
    "        val = 3\n",
    "    else:\n",
    "        val = 0\n",
    "\n",
    "    model, binarizer, label_encoder = load_model_from_files(model_paths[i], binarizer_paths[val], label_encoder_paths[val])\n",
    "    \n",
    "    for j, symptoms in enumerate(sample_symptoms_list):\n",
    "        raw_symptoms = sample_symptoms_raw_list[j]\n",
    "        predicted_diseases_with_confidence = predict_disease(model, binarizer, label_encoder, symptoms, raw_symptoms, model_path=model_paths[i])\n",
    "        top_prediction = predicted_diseases_with_confidence[0]\n",
    "        \n",
    "        # Notation to mark if the prediction was correct or incorrect\n",
    "        notation = \"Incorrect\"\n",
    "        if top_prediction[0] == sample_diseases[j]:\n",
    "            notation = \"Correct\"\n",
    "            correct_predictions[i] += 1\n",
    "        \n",
    "        # Add the top prediction for this model to the disease's row\n",
    "        table_data[j].append(f\"{top_prediction[0]} ({top_prediction[1]*100:.2f}%) {notation}\")\n",
    "\n",
    "# Now, we need to prefix each row with the code and the actual disease name\n",
    "for j in range(len(sample_symptoms_list)):\n",
    "    disease_name = get_disease_name_from_code(sample_diseases[j])\n",
    "    table_data[j].insert(0, disease_name)\n",
    "    table_data[j].insert(0, sample_diseases[j])  # Inserting code as a separate column entry\n",
    "\n",
    "headers = ['Code']  + ['Actual Disease'] +  [f\"Model {i+1}\" for i in range(len(model_paths))]\n",
    "\n",
    "print(tabulate(table_data, headers=headers))\n",
    "\n",
    "# Compute and display accuracy for each model\n",
    "total_predictions = len(sample_symptoms_list)\n",
    "for i, correct in enumerate(correct_predictions):\n",
    "    accuracy = (correct / total_predictions) * 100\n",
    "    print(f\"Accuracy for Model {i+1}: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
