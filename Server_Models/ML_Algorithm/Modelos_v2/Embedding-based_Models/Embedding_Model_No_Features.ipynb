{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/guilherme/Documents/GitHub/Tese/Documentation/Dataset_Augmentation/Raw_Augmented_database_13_10_2023.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m     19\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/guilherme/Documents/GitHub/Tese/Documentation/Dataset_Augmentation/Raw_Augmented_database_13_10_2023.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAugmented Data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.11/3.11.6_1/lib/python3.11/site-packages/pandas/io/excel/_base.py:504\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    503\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m     )\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.11/3.11.6_1/lib/python3.11/site-packages/pandas/io/excel/_base.py:1563\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1563\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1568\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1570\u001b[0m         )\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.11/3.11.6_1/lib/python3.11/site-packages/pandas/io/excel/_base.py:1419\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1417\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1419\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1421\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1422\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1423\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.11/3.11.6_1/lib/python3.11/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/guilherme/Documents/GitHub/Tese/Documentation/Dataset_Augmentation/Raw_Augmented_database_13_10_2023.xlsx'"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "file_path = \"/home/guilherme/Documents/Github/Tese/Documentation/Dataset_Augmentation/Raw_Augmented_database_13_10_2023.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Augmented Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the data: Create a list of symptom sequences for each disease\n",
    "symptom_cols = [f\"Symptom_{i}\" for i in range(1, 26)]\n",
    "symptoms_data = df[symptom_cols].values.tolist()\n",
    "\n",
    "# Filtering out NaN values from symptoms\n",
    "symptoms_data = [list(filter(lambda v: v==v, lst)) for lst in symptoms_data]  # v==v is a trick to check for NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train Word2Vec embeddings on these sequences\n",
    "\n",
    "embedding_size = 128\n",
    "w2v_model = Word2Vec(sentences=symptoms_data, vector_size=embedding_size, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Exploratory data analysis (EDA)\n",
    "print(df.describe())\n",
    "print(df.info())\n",
    "\n",
    "# %%\n",
    "# EDA Step 2: Distribution of the target variable (ICD 11 codes)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "top_n_diseases = 50  # You can change this as needed\n",
    "sns.countplot(y=df[\"ICD 11\"], order=df[\"ICD 11\"].value_counts().head(top_n_diseases).index)\n",
    "plt.title(\"Distribution of Top ICD 11 Codes\")\n",
    "plt.ylabel(\"ICD 11 Codes\")\n",
    "plt.xlabel(\"Number of Occurrences\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# EDA Step 3: Distribution of the number of symptoms per record\n",
    "\n",
    "symptom_counts = df[symptom_cols].count(axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(symptom_counts, kde=False, bins=30, color=\"skyblue\")\n",
    "\n",
    "# Determine where 80% of the data falls\n",
    "sorted_symptom_counts = sorted(symptom_counts)\n",
    "index_10_percent = int(0.10 * len(sorted_symptom_counts))\n",
    "index_90_percent = int(0.90 * len(sorted_symptom_counts))\n",
    "lower_bound = sorted_symptom_counts[index_10_percent]\n",
    "upper_bound = sorted_symptom_counts[index_90_percent]\n",
    "\n",
    "# Highlight the range where 80% of occurrences fall\n",
    "plt.axvspan(lower_bound, upper_bound, color='red', alpha=0.3)\n",
    "plt.title(\"Distribution of Number of Symptoms per Record\")\n",
    "plt.xlabel(\"Number of Symptoms\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# EDA Step 4: Visualization of Word2Vec embeddings using dimensionality reduction (TSNE)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "top_n_symptoms = 40\n",
    "top_symptoms = pd.Series([item for sublist in symptoms_data for item in sublist]).value_counts().head(top_n_symptoms).index.tolist()\n",
    "top_symptoms_embeddings = np.array([w2v_model.wv[symptom] for symptom in top_symptoms])\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)  # Adjust perplexity based on data size\n",
    "reduced_embeddings = tsne.fit_transform(top_symptoms_embeddings)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "scatter = sns.scatterplot(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1], alpha=0.8, s=80, hue=top_symptoms, palette='viridis')\n",
    "plt.title(\"Visualization of Word2Vec Embeddings using TSNE\")\n",
    "plt.xlabel(\"TSNE Dimension 1\")\n",
    "plt.ylabel(\"TSNE Dimension 2\")\n",
    "scatter.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"Word2Vec_Visualization.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Convert the symptom sequences into embeddings\n",
    "\n",
    "def symptoms_to_embedding(symptoms, model, embedding_size):\n",
    "    embedding_matrix = np.zeros((len(symptoms), embedding_size))\n",
    "    for i, symptom in enumerate(symptoms):\n",
    "        if symptom in model.wv:\n",
    "            embedding_matrix[i] = model.wv[str(symptom)]\n",
    "    return embedding_matrix\n",
    "\n",
    "X_embedded = np.array([symptoms_to_embedding(patient_symptoms, w2v_model, embedding_size) for patient_symptoms in symptoms_data])\n",
    "X_padded = pad_sequences(X_embedded, padding='post')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df[\"ICD 11\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Class balancing through oversampling\n",
    "\n",
    "# Class Imbalance: If certain classes (diseases) have significantly more samples than others, \n",
    "# you might need to address this imbalance using techniques like oversampling, undersampling,\n",
    "#  or using metrics that are sensitive to imbalances\n",
    "\n",
    "\n",
    "# Addressing class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_padded.reshape(X_padded.shape[0], -1), y_encoded)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=42, stratify=y_resampled)\n",
    "\n",
    "\n",
    "# Compute the class weights.\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "unique_classes = np.unique(y_encoded)\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_encoded)\n",
    "class_weight_dict = dict(zip(unique_classes, class_weights))\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_encoded = to_categorical(y_train, num_classes=len(unique_classes))\n",
    "y_test_encoded = to_categorical(y_test, num_classes=len(unique_classes))\n",
    "\n",
    "max_sequence_length = X_padded.shape[1]\n",
    "X_train = X_train.reshape(-1, max_sequence_length, 128)\n",
    "X_test = X_test.reshape(-1, max_sequence_length, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Creating the model architecture (LSTM with embeddings)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "n_classes = len(np.unique(y_encoded))\n",
    "print(n_classes)\n",
    "# Define the model-building function for LSTM\n",
    "def build_lstm_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(units=hp.Int('lstm_units_1', min_value=64, max_value=256, step=64), \n",
    "                   input_shape=(X_padded.shape[1], w2v_model.vector_size), \n",
    "                   return_sequences=True))\n",
    "\n",
    "    model.add(LSTM(units=hp.Int('lstm_units_2', min_value=64, max_value=256, step=64)))\n",
    "\n",
    "    model.add(Dense(units=hp.Int('dense_units_1', min_value=256, max_value=1024, step=256), \n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(units=hp.Int('dense_units_2', min_value=128, max_value=512, step=128), \n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_lstm_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=2,\n",
    "    directory='hyperparam_search',\n",
    "    project_name='disease_prediction_lstm'\n",
    ")\n",
    "\n",
    "# Search for the best model\n",
    "tuner.search(X_train, y_train_encoded, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Get the best hyperparameters and build the model\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "print(best_hyperparameters.values)\n",
    "model = tuner.hypermodel.build(best_hyperparameters)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5. Training the Model\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, validation_data=(X_test, y_test_encoded), epochs=20, batch_size=64, class_weight=class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Model Evaluation\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Accuracy of the model: {accuracy * 100:.2f}%\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Visualize the training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Model Deployment\n",
    "import joblib\n",
    "\n",
    "# Save the model, label encoder, and binarizer for later use\n",
    "model_filename = 'Raw_Embedded_model.pkl'\n",
    "w2v_model_filename = 'raw_w2v_model.pkl'\n",
    "X_padded_filename = 'raw_X_padded.pkl'\n",
    "# binarizer_filename = 'no_features_symptoms_binarizer.pkl'\n",
    "\n",
    "joblib.dump(model, model_filename)\n",
    "joblib.dump(w2v_model, w2v_model_filename)\n",
    "joblib.dump(X_padded, X_padded_filename)\n",
    "\n",
    "loaded_model = joblib.load(model_filename)\n",
    "w2v_model_encoder = joblib.load(w2v_model_filename)\n",
    "loaded_binarizer = joblib.load(X_padded_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Making Predictions (Optional)\n",
    "\n",
    "def predict_top_diseases_with_confidence(symptom_codes, top_n=5):\n",
    "    # Convert input symptoms to embeddings\n",
    "    embedded_symptoms = symptoms_to_embedding(symptom_codes, w2v_model_encoder, embedding_size)\n",
    "    embedded_symptoms = np.expand_dims(embedded_symptoms, 0)  # Add batch dimension\n",
    "    \n",
    "    # Pad the embedded symptoms\n",
    "    padded_symptoms = pad_sequences(embedded_symptoms, maxlen=loaded_binarizer.shape[1], padding='post')\n",
    "\n",
    "    # Predict using the model\n",
    "    prediction = loaded_model.predict(padded_symptoms)\n",
    "\n",
    "    # Get top N class indices based on highest probabilities\n",
    "    top_n_indices = np.argsort(prediction[0])[-top_n:][::-1]\n",
    "    top_n_probabilities = prediction[0][top_n_indices]\n",
    "\n",
    "    # Decode these indices to get the ICD 11 codes\n",
    "    top_n_icd_codes = label_encoder.inverse_transform(top_n_indices)\n",
    "\n",
    "    # Zip together the ICD codes and their respective probabilities\n",
    "    results = list(zip(top_n_icd_codes, top_n_probabilities))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "symptom_input = ['Fatigue', 'Shoulder pain', 'Leg pain', 'Muscle pain', 'Hip pain', 'Irregular heartbeat', 'Stiffness all over', 'Temper problems', 'Morning stiffness', 'Limited range of motion', 'Anorexia'] # FA22\tPolymyalgia rheumatica\n",
    "predictions_with_confidence = predict_top_diseases_with_confidence(symptom_input)\n",
    "for icd, confidence in predictions_with_confidence:\n",
    "    print(f\"Predicted disease (ICD 11 Code): {icd} with confidence: {confidence:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
