{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Loading the Data and required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/home/guilherme/Documents/Github/Tese/Documentation/Dataset_Augmentation/Augmented_database_29_09_2023.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Original Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Data Transformation and Encoding\n",
    "\n",
    "# Replace NaN with 'None' placeholder\n",
    "symptom_cols = [f'Symptom_{i}' for i in range(1, 26)]\n",
    "df[symptom_cols] = df[symptom_cols].fillna('None')\n",
    "df['symptoms'] = df[symptom_cols].apply(lambda row: [symptom for symptom in row if symptom != 'None'], axis=1)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "X = mlb.fit_transform(df['symptoms'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['ICD 11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Train/Test Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Introducing SMOTE\n",
    "smote = SMOTE(k_neighbors=4, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Hyperparameter tunning\n",
    "\n",
    "# Import the necessary libraries\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import shap\n",
    "\n",
    "# Calculate the number of classes\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('input_units', min_value=256, max_value=1024, step=256),\n",
    "                    activation='relu',\n",
    "                    input_shape=(X_train_resampled.shape[1],)))\n",
    "\n",
    "    for i in range(hp.Int('n_layers', 1, 4)):\n",
    "        model.add(Dense(units=hp.Int(f'hidden_units_{i}', min_value=128, max_value=512, step=128),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='hyperparam_search',\n",
    "    project_name='disease_prediction'\n",
    ")\n",
    "\n",
    "# Search for the best model\n",
    "tuner.search(X_train_resampled, y_train_resampled, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Get the best hyperparameters and build the model\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "model = tuner.hypermodel.build(best_hyperparameters)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5. Training the Model\n",
    "\n",
    "history = model.fit(X_train_resampled, y_train_resampled, epochs=20, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 6. Evaluating the Model\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracy of the model: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict class labels for the test set\n",
    "y_pred_probabilities = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "# Ensure all classes are accounted for in the classification report\n",
    "all_classes = np.arange(len(label_encoder.classes_))\n",
    "report = classification_report(y_test, y_pred_classes, labels=all_classes, target_names=label_encoder.classes_)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\",\n",
    "            cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Visualization\n",
    "\n",
    "# We can also visualize the training progress.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting accuracy and loss over epochs\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')  # X-axis label\n",
    "plt.ylabel('Accuracy')  # Y-axis label\n",
    "plt.legend()\n",
    "plt.title('Accuracy Over Epochs')\n",
    "\n",
    "# Plotting loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')  # X-axis label\n",
    "plt.ylabel('Loss')  # Y-axis label\n",
    "plt.legend()\n",
    "plt.title('Loss Over Epochs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Model Deployment\n",
    "import joblib\n",
    "\n",
    "# Save the model, label encoder, and binarizer for later use\n",
    "model_filename = 'NN_no_features.pkl'\n",
    "label_encoder_filename = 'no_features_label_encoder.pkl'\n",
    "binarizer_filename = 'no_features_symptoms_binarizer.pkl'\n",
    "\n",
    "joblib.dump(model, model_filename)\n",
    "joblib.dump(label_encoder, label_encoder_filename)\n",
    "joblib.dump(mlb, binarizer_filename)\n",
    "\n",
    "loaded_model = joblib.load(model_filename)\n",
    "loaded_label_encoder = joblib.load(label_encoder_filename)\n",
    "loaded_binarizer = joblib.load(binarizer_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Disease prediction\n",
    "\n",
    "def predict_disease(symptoms_list, top_n=2):\n",
    "    \"\"\"\n",
    "    Given a list of symptoms (ICD-11 codes), predict the top potential diseases along with their confidence.\n",
    "    \"\"\"\n",
    "    # Transform the symptoms list into the appropriate binary vector format\n",
    "    symptoms_encoded = mlb.transform([symptoms_list])  # `mlb` was previously used for multilabel binarization\n",
    "\n",
    "    # Predict the probability distribution over classes using the trained model\n",
    "    disease_probabilities = loaded_model.predict(symptoms_encoded)\n",
    "\n",
    "    # Get indices of the top_n classes\n",
    "    top_indices = np.argsort(disease_probabilities[0])[-top_n:][::-1]\n",
    "\n",
    "    # Decode these indices to get the actual disease codes\n",
    "    top_diseases = label_encoder.inverse_transform(top_indices)\n",
    "    \n",
    "    # Extract their corresponding probabilities\n",
    "    top_probabilities = disease_probabilities[0][top_indices]\n",
    "\n",
    "    return list(zip(top_diseases, top_probabilities))\n",
    "\n",
    "# Test the prediction function\n",
    "sample_symptoms = ['MC15', '9D9Z', '9D90.6', '9C80.0', 'LD20.4', '8A68.Z', '9B73.3', '9B65.2', '1D01.Y', 'MA01.Z'] # 1F57.Z\tToxoplasmosis\n",
    "predicted_diseases_with_confidence = predict_disease(sample_symptoms)\n",
    "\n",
    "for disease, confidence in predicted_diseases_with_confidence:\n",
    "    print(f\"Disease: {disease} with confidence: {confidence*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Define the feature names\n",
    "feature_names = mlb.classes_\n",
    "\n",
    "# Get the encoding for sample symptoms\n",
    "new_symptoms_encoded = mlb.transform([sample_symptoms])\n",
    "\n",
    "# Initialize the SHAP GradientExplainer with training data and compute SHAP values for test data\n",
    "explainer = shap.GradientExplainer(model, X_train_resampled[:100])\n",
    "shap_values = explainer.shap_values(new_symptoms_encoded)\n",
    "\n",
    "# If SHAP values are in the form of tf.Tensor, convert them to numpy arrays\n",
    "shap_values = [s.numpy() if isinstance(s, tf.Tensor) else s for s in shap_values]\n",
    "shap.initjs()\n",
    "\n",
    "# Fetch indices for the sample symptoms\n",
    "new_symptoms_indices = [all_possible_symptoms.tolist().index(symptom) for symptom in sample_symptoms]\n",
    "\n",
    "\n",
    "# Loop over the predicted diseases\n",
    "for disease, _ in predicted_diseases_with_confidence:\n",
    "    transformed_label = label_encoder.transform([disease])[0]\n",
    "    print(f\"Transformed label for {disease}:\", transformed_label)\n",
    "\n",
    "    class_shap_values = shap_values[transformed_label]\n",
    "\n",
    "    # Filter class_shap_values based on sample_symptoms\n",
    "    filtered_shap_values = class_shap_values[0, new_symptoms_indices].reshape(1, -1)\n",
    "    filtered_symptoms = np.array(all_possible_symptoms)[new_symptoms_indices]\n",
    "\n",
    "    # Adjust the figure size and plot\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    shap.summary_plot(filtered_shap_values, filtered_symptoms, color=\"#90EE90\", plot_type=\"bar\", title=f\"SHAP values for {disease}\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop over the predicted diseases\n",
    "for disease, _ in predicted_diseases_with_confidence:\n",
    "    transformed_label = label_encoder.transform([disease])[0]\n",
    "    print(f\"Transformed label for {disease}:\", transformed_label)\n",
    "    \n",
    "    class_shap_values = shap_array[transformed_label]\n",
    "    \n",
    "    # Filter class_shap_values based on sample_symptoms\n",
    "    filtered_shap_values = class_shap_values[0, new_symptoms_indices].reshape(1, -1)\n",
    "    filtered_symptoms = np.array(all_possible_symptoms)[new_symptoms_indices]\n",
    "    \n",
    "    # Get feature values for color\n",
    "    feature_values = new_symptoms_encoded[0, new_symptoms_indices].reshape(1, -1)\n",
    "    \n",
    "    # Adjust the figure size and plot with the color bar\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    shap.summary_plot(filtered_shap_values, feature_names=filtered_symptoms, title=f\"SHAP values for {disease}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_names = mlb.classes_\n",
    "\n",
    "explainer = shap.GradientExplainer(model, X_train_resampled[:100])\n",
    "shap_values = explainer.shap_values(X_test[0:1])\n",
    "\n",
    "# Compute expected_value\n",
    "expected_value = model.predict(X_train_resampled[:100]).mean()\n",
    "\n",
    "shap_values = [s.numpy() if isinstance(s, tf.Tensor) else s for s in shap_values]\n",
    "shap.initjs()\n",
    "\n",
    "# This gets the first set of shap values (for the first instance)\n",
    "single_shap_values = shap_values[0][0] if isinstance(shap_values[0], list) else shap_values[0]\n",
    "\n",
    "print(\"Type of expected_value:\", type(expected_value))\n",
    "print(\"Value of expected_value:\", expected_value)\n",
    "print(\"Shape of single_shap_values:\", np.shape(single_shap_values))\n",
    "# print(\"First few elements of single_shap_values:\", single_shap_values[:5])\n",
    "\n",
    "# shap.force_plot(expected_value, single_shap_values, X_test[0:1], feature_names=mlb.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(shap_values[i].shape)\n",
    "print(X_test.shape)\n",
    "print(mlb.classes_[:5])\n",
    "\n",
    "first_instance = X_test[0]\n",
    "# Reshape the first_instance to make it 2D\n",
    "reshaped_instance = first_instance.reshape(1, -1)\n",
    "decoded_features = mlb.inverse_transform(reshaped_instance)[0]\n",
    "print(decoded_features)\n",
    "\n",
    "\n",
    "first_corresponding_value = y_test[0]\n",
    "first_corresponding_value_decoded = label_encoder.inverse_transform([first_corresponding_value])\n",
    "print(first_corresponding_value_decoded)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"First instance from X_test:\", first_instance)\n",
    "# print(\"Corresponding value from y_test:\", first_corresponding_value)\n",
    "\n",
    "\n",
    "# Calculate SHAP values for a larger subset or the entire test dataset\n",
    "for i in range(5): # First 5 instances of the test set# n_classes\n",
    "  # shap.summary_plot(shap_values[i], X_test[i].reshape(1, -1), feature_names=mlb.classes_, title=f\"Class {i} - SHAP Summary\")\n",
    "  print (y_test[i])\n",
    "  shap.summary_plot(shap_values[i], X_test[i].reshape(1, -1), feature_names=mlb.classes_, title=f\"Class {label_encoder.inverse_transform([i])[0]} - SHAP Summary\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
