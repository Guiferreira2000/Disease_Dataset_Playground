{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Loading the Data and required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(\"/home/guilherme/Documents/GitHub/Tese/MDCompass/\")\n",
    "\n",
    "from ML_Algorithm.Functions.compute_symptom_relatedness import compute_relatedness, compute_relatedness_matrix\n",
    "\n",
    "# Loading the severity scores\n",
    "with open(\"/home/guilherme/Documents/GitHub/Tese/MDCompass/ML_Algorithm/json_files/severity_scores.json\", \"r\") as f:\n",
    "    severity_mapping = json.load(f)\n",
    "    \n",
    "# Load the dataset\n",
    "file_path = \"/home/guilherme/Documents/GitHub/Tese/Documentation/Dataset_Augmentation/Augmented_database_29_09_2023.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Augmented Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Transformation and Encoding\n",
    "\n",
    "# Replace NaN with 'None' placeholder\n",
    "symptom_cols = [f'Symptom_{i}' for i in range(1, 26)]\n",
    "df[symptom_cols] = df[symptom_cols].fillna('None')\n",
    "df['symptoms'] = df[symptom_cols].apply(lambda row: [symptom for symptom in row if symptom != 'None'], axis=1)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "X = mlb.fit_transform(df['symptoms'])\n",
    "\n",
    "# Insert the additional features calculation here\n",
    "def get_severity_scores(symptom_list):\n",
    "    return [severity_mapping.get(symptom, 0) for symptom in symptom_list]\n",
    "\n",
    "\n",
    "# Calculating additional features\n",
    "df['relatedness_values'] = df['symptoms'].apply(compute_relatedness_matrix)\n",
    "df['severity_values'] = df['symptoms'].apply(get_severity_scores)\n",
    "\n",
    "# Padding the relatedness matrix to a consistent shape\n",
    "max_dim = max([len(x) for x in df['relatedness_values']])\n",
    "\n",
    "def pad_matrix_to_shape(matrix, max_dim):\n",
    "    # Determine how much to pad\n",
    "    pad_dim = max_dim - matrix.shape[0]\n",
    "    \n",
    "    # Pad rows\n",
    "    matrix = np.pad(matrix, ((0, pad_dim), (0, pad_dim)), 'constant')\n",
    "    return matrix\n",
    "\n",
    "# After calculating max_dim in the training phase\n",
    "with open('max_dim.json', 'w') as f:\n",
    "    json.dump({\"max_dim\": max_dim}, f)\n",
    "\n",
    "\n",
    "# Apply padding to each matrix\n",
    "df['relatedness_values_padded'] = df['relatedness_values'].apply(lambda x: pad_matrix_to_shape(x, max_dim))\n",
    "\n",
    "# Padding the severity values to have the same length\n",
    "padded_severity = pad_sequences(df['severity_values'], padding='post', dtype='float32', maxlen=max_dim)\n",
    "\n",
    "# Now, reshaping the relatedness and severity features\n",
    "relatedness_features = np.array(df['relatedness_values_padded'].tolist()).reshape(len(df), -1)\n",
    "\n",
    "severity_features = padded_severity.reshape(len(df), -1)\n",
    "\n",
    "# Stacking them up with the binarized symptoms\n",
    "X = np.hstack((X, relatedness_features, severity_features))\n",
    "# X = np.hstack((X, relatedness_features))\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['ICD 11'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Train/Test Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Introducing SMOTE\n",
    "smote = SMOTE(k_neighbors=4, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Hyperparameter tunning\n",
    "\n",
    "# Import the necessary libraries\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Calculate the number of classes\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('input_units', min_value=256, max_value=1024, step=256),\n",
    "                    activation='relu', \n",
    "                    input_shape=(X_train_resampled.shape[1],)))\n",
    "    \n",
    "    for i in range(hp.Int('n_layers', 1, 4)):\n",
    "        model.add(Dense(units=hp.Int(f'hidden_units_{i}', min_value=128, max_value=512, step=128),\n",
    "                        activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,  # No lambda function required here\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='hyperparam_search',\n",
    "    project_name='disease_prediction'\n",
    ")\n",
    "\n",
    "# Summarize the search space\n",
    "# tuner.search_space_summary()\n",
    "\n",
    "# Search for the best model\n",
    "tuner.search(X_train_resampled, y_train_resampled, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Get the best hyperparameters and build the model\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "print(best_hyperparameters.values)\n",
    "model = tuner.hypermodel.build(best_hyperparameters)\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5. Training the Model\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, \n",
    "                    validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# history = model.fit(X_train_resampled, y_train_resampled, epochs=20, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 6. Evaluating the Model\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracy of the model: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict class labels for the test set\n",
    "y_pred_probabilities = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "# Ensure all classes are accounted for in the classification report\n",
    "all_classes = np.arange(len(label_encoder.classes_))\n",
    "report = classification_report(y_test, y_pred_classes, labels=all_classes, target_names=label_encoder.classes_)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Model Deployment\n",
    "import joblib\n",
    "\n",
    "# Save the model, label encoder, and binarizer for later use\n",
    "model_filename = 'NN_with_features.pkl'\n",
    "label_encoder_filename = 'disease_label_encoder.pkl'\n",
    "binarizer_filename = 'symptoms_binarizer.pkl'\n",
    "\n",
    "joblib.dump(model, model_filename)\n",
    "joblib.dump(label_encoder, label_encoder_filename)\n",
    "joblib.dump(mlb, binarizer_filename)\n",
    "\n",
    "loaded_model = joblib.load(model_filename)\n",
    "loaded_label_encoder = joblib.load(label_encoder_filename)\n",
    "loaded_binarizer = joblib.load(binarizer_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_disease_nn(symptoms_list):\n",
    "    \"\"\"\n",
    "    Given a list of symptoms (ICD-11 codes), predict potential diseases using the trained neural network.\n",
    "    \"\"\"\n",
    "    # Transform the symptoms list into the appropriate binary vector format and other preprocessing\n",
    "    symptoms_encoded = loaded_binarizer.transform([symptoms_list])\n",
    "    \n",
    "    # Calculate the relatedness and severity features\n",
    "    relatedness_values = compute_relatedness_matrix(symptoms_list)\n",
    "    severity_values = get_severity_scores(symptoms_list)\n",
    "\n",
    "    # Flatten the relatedness values matrix\n",
    "    flattened_relatedness = relatedness_values.reshape(-1)\n",
    "    \n",
    "    # Pad the sequences\n",
    "    padded_relatedness = pad_sequences([flattened_relatedness], maxlen=max_dim*max_dim, padding='post', dtype='float32').reshape(1, -1)\n",
    "    padded_severity = pad_sequences([severity_values], maxlen=max_dim, padding='post', dtype='float32').reshape(1, -1)\n",
    "\n",
    "    # Combine all the features\n",
    "    # combined_features = np.hstack((symptoms_encoded, padded_relatedness, padded_severity))\n",
    "    combined_features = np.hstack((symptoms_encoded, padded_relatedness))\n",
    "\n",
    "    \n",
    "    # Predict using the trained model\n",
    "    disease_probs = loaded_model.predict(combined_features)\n",
    "    \n",
    "    # Sort probabilities and get top 5 indices\n",
    "    top_5_disease_indices = np.argsort(disease_probs[0])[::-1][:5]\n",
    "    \n",
    "    # Decode the predicted disease codes\n",
    "    top_5_disease_predictions = loaded_label_encoder.inverse_transform(top_5_disease_indices)\n",
    "\n",
    "    # Extract the associated probabilities\n",
    "    top_5_confidences = disease_probs[0][top_5_disease_indices]\n",
    "    \n",
    "    results = [(disease, f\"{confidence * 100:.2f}%\") for disease, confidence in zip(top_5_disease_predictions, top_5_confidences)]\n",
    "\n",
    "    return results\n",
    "\n",
    "sample_symptoms = ['MC15', '9D9Z', '9D90.6', '9C80.0', 'LD20.4', '8A68.Z', '9B73.3', '9B65.2', '1D01.Y', 'MA01.Z']\n",
    "predicted_diseases = predict_disease_nn(sample_symptoms)\n",
    "\n",
    "print(\"Given the symptoms, the top 5 predicted diseases are:\")\n",
    "for disease, confidence in predicted_diseases:\n",
    "    print(f\"Disease ICD-11 code: {disease} with confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\",\n",
    "            cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Visualization\n",
    "\n",
    "# We can also visualize the training progress.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting accuracy and loss over epochs\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plotting accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Over Epochs')\n",
    "\n",
    "# Plotting loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Over Epochs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
