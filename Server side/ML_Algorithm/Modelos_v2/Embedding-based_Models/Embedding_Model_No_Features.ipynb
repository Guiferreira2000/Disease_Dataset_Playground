{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "file_path = \"/home/guilherme/Documents/GitHub/Tese/Documentation/Dataset_Augmentation/Raw_Augmented_database_13_10_2023.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Augmented Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the data: Create a list of symptom sequences for each disease\n",
    "symptom_cols = [f\"Symptom_{i}\" for i in range(1, 26)]\n",
    "symptoms_data = df[symptom_cols].values.tolist()\n",
    "\n",
    "# Filtering out NaN values from symptoms\n",
    "symptoms_data = [list(filter(lambda v: v==v, lst)) for lst in symptoms_data]  # v==v is a trick to check for NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train Word2Vec embeddings on these sequences\n",
    "\n",
    "embedding_size = 128\n",
    "w2v_model = Word2Vec(sentences=symptoms_data, vector_size=embedding_size, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Exploratory data analysis (EDA)\n",
    "print(df.describe())\n",
    "print(df.info())\n",
    "\n",
    "# %%\n",
    "# EDA Step 2: Distribution of the target variable (ICD 11 codes)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "top_n_diseases = 50  # You can change this as needed\n",
    "sns.countplot(y=df[\"ICD 11\"], order=df[\"ICD 11\"].value_counts().head(top_n_diseases).index)\n",
    "plt.title(\"Distribution of Top ICD 11 Codes\")\n",
    "plt.ylabel(\"ICD 11 Codes\")\n",
    "plt.xlabel(\"Number of Occurrences\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# EDA Step 3: Distribution of the number of symptoms per record\n",
    "\n",
    "symptom_counts = df[symptom_cols].count(axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(symptom_counts, kde=False, bins=30, color=\"skyblue\")\n",
    "\n",
    "# Determine where 80% of the data falls\n",
    "sorted_symptom_counts = sorted(symptom_counts)\n",
    "index_10_percent = int(0.10 * len(sorted_symptom_counts))\n",
    "index_90_percent = int(0.90 * len(sorted_symptom_counts))\n",
    "lower_bound = sorted_symptom_counts[index_10_percent]\n",
    "upper_bound = sorted_symptom_counts[index_90_percent]\n",
    "\n",
    "# Highlight the range where 80% of occurrences fall\n",
    "plt.axvspan(lower_bound, upper_bound, color='red', alpha=0.3)\n",
    "plt.title(\"Distribution of Number of Symptoms per Record\")\n",
    "plt.xlabel(\"Number of Symptoms\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# EDA Step 4: Visualization of Word2Vec embeddings using dimensionality reduction (TSNE)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "top_n_symptoms = 40\n",
    "top_symptoms = pd.Series([item for sublist in symptoms_data for item in sublist]).value_counts().head(top_n_symptoms).index.tolist()\n",
    "top_symptoms_embeddings = np.array([w2v_model.wv[symptom] for symptom in top_symptoms])\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)  # Adjust perplexity based on data size\n",
    "reduced_embeddings = tsne.fit_transform(top_symptoms_embeddings)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "scatter = sns.scatterplot(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1], alpha=0.8, s=80, hue=top_symptoms, palette='viridis')\n",
    "plt.title(\"Visualization of Word2Vec Embeddings using TSNE\")\n",
    "plt.xlabel(\"TSNE Dimension 1\")\n",
    "plt.ylabel(\"TSNE Dimension 2\")\n",
    "scatter.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='small')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"Word2Vec_Visualization.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Convert the symptom sequences into embeddings\n",
    "\n",
    "def symptoms_to_embedding(symptoms, model, embedding_size):\n",
    "    embedding_matrix = np.zeros((len(symptoms), embedding_size))\n",
    "    for i, symptom in enumerate(symptoms):\n",
    "        if symptom in model.wv:\n",
    "            embedding_matrix[i] = model.wv[str(symptom)]\n",
    "    return embedding_matrix\n",
    "\n",
    "X_embedded = np.array([symptoms_to_embedding(patient_symptoms, w2v_model, embedding_size) for patient_symptoms in symptoms_data])\n",
    "X_padded = pad_sequences(X_embedded, padding='post')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df[\"ICD 11\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Class balancing through oversampling\n",
    "\n",
    "# Class Imbalance: If certain classes (diseases) have significantly more samples than others, \n",
    "# you might need to address this imbalance using techniques like oversampling, undersampling,\n",
    "#  or using metrics that are sensitive to imbalances\n",
    "\n",
    "\n",
    "# Addressing class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_padded.reshape(X_padded.shape[0], -1), y_encoded)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=42, stratify=y_resampled)\n",
    "\n",
    "\n",
    "# Compute the class weights.\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "unique_classes = np.unique(y_encoded)\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_encoded)\n",
    "class_weight_dict = dict(zip(unique_classes, class_weights))\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_encoded = to_categorical(y_train, num_classes=len(unique_classes))\n",
    "y_test_encoded = to_categorical(y_test, num_classes=len(unique_classes))\n",
    "\n",
    "max_sequence_length = X_padded.shape[1]\n",
    "X_train = X_train.reshape(-1, max_sequence_length, 128)\n",
    "X_test = X_test.reshape(-1, max_sequence_length, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Creating the model architecture (LSTM with embeddings)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "n_classes = len(np.unique(y_encoded))\n",
    "print(n_classes)\n",
    "# Define the model-building function for LSTM\n",
    "def build_lstm_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(units=hp.Int('lstm_units_1', min_value=64, max_value=256, step=64), \n",
    "                   input_shape=(X_padded.shape[1], w2v_model.vector_size), \n",
    "                   return_sequences=True))\n",
    "\n",
    "    model.add(LSTM(units=hp.Int('lstm_units_2', min_value=64, max_value=256, step=64)))\n",
    "\n",
    "    model.add(Dense(units=hp.Int('dense_units_1', min_value=256, max_value=1024, step=256), \n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(units=hp.Int('dense_units_2', min_value=128, max_value=512, step=128), \n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_lstm_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=2,\n",
    "    directory='hyperparam_search',\n",
    "    project_name='disease_prediction_lstm'\n",
    ")\n",
    "\n",
    "# Search for the best model\n",
    "tuner.search(X_train, y_train_encoded, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Get the best hyperparameters and build the model\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "print(best_hyperparameters.values)\n",
    "model = tuner.hypermodel.build(best_hyperparameters)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5. Training the Model\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, validation_data=(X_test, y_test_encoded), epochs=20, batch_size=64, class_weight=class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Model Evaluation\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Accuracy of the model: {accuracy * 100:.2f}%\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Visualize the training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Model Deployment\n",
    "import joblib\n",
    "\n",
    "# Save the model, label encoder, and binarizer for later use\n",
    "model_filename = 'Raw_Embedded_model.pkl'\n",
    "w2v_model_filename = 'raw_w2v_model.pkl'\n",
    "X_padded_filename = 'raw_X_padded.pkl'\n",
    "# binarizer_filename = 'no_features_symptoms_binarizer.pkl'\n",
    "\n",
    "joblib.dump(model, model_filename)\n",
    "joblib.dump(w2v_model, w2v_model_filename)\n",
    "joblib.dump(X_padded, X_padded_filename)\n",
    "\n",
    "loaded_model = joblib.load(model_filename)\n",
    "w2v_model_encoder = joblib.load(w2v_model_filename)\n",
    "loaded_binarizer = joblib.load(X_padded_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Making Predictions (Optional)\n",
    "\n",
    "def predict_top_diseases_with_confidence(symptom_codes, top_n=5):\n",
    "    # Convert input symptoms to embeddings\n",
    "    embedded_symptoms = symptoms_to_embedding(symptom_codes, w2v_model_encoder, embedding_size)\n",
    "    embedded_symptoms = np.expand_dims(embedded_symptoms, 0)  # Add batch dimension\n",
    "    \n",
    "    # Pad the embedded symptoms\n",
    "    padded_symptoms = pad_sequences(embedded_symptoms, maxlen=loaded_binarizer.shape[1], padding='post')\n",
    "\n",
    "    # Predict using the model\n",
    "    prediction = loaded_model.predict(padded_symptoms)\n",
    "\n",
    "    # Get top N class indices based on highest probabilities\n",
    "    top_n_indices = np.argsort(prediction[0])[-top_n:][::-1]\n",
    "    top_n_probabilities = prediction[0][top_n_indices]\n",
    "\n",
    "    # Decode these indices to get the ICD 11 codes\n",
    "    top_n_icd_codes = label_encoder.inverse_transform(top_n_indices)\n",
    "\n",
    "    # Zip together the ICD codes and their respective probabilities\n",
    "    results = list(zip(top_n_icd_codes, top_n_probabilities))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "symptom_input = ['Fatigue', 'Shoulder pain', 'Leg pain', 'Muscle pain', 'Hip pain', 'Irregular heartbeat', 'Stiffness all over', 'Temper problems', 'Morning stiffness', 'Limited range of motion', 'Anorexia'] # FA22\tPolymyalgia rheumatica\n",
    "predictions_with_confidence = predict_top_diseases_with_confidence(symptom_input)\n",
    "for icd, confidence in predictions_with_confidence:\n",
    "    print(f\"Predicted disease (ICD 11 Code): {icd} with confidence: {confidence:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
