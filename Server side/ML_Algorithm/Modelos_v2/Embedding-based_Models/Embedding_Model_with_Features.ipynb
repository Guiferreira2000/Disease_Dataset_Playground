{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, Dropout\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"/home/guilherme/Documents/GitHub/Tese/Documentation/Dataset_Augmentation/Augmented_database_22_09_2023.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Augmented Data\")\n",
    "\n",
    "with open(\"/home/guilherme/Documents/GitHub/Tese/MDCompass/ML_Algorithm/json_files/severity_scores.json\", \"r\") as f:\n",
    "    severity_mapping = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add the features functions\n",
    "\n",
    "def get_severity_scores(symptom_list):\n",
    "    return [severity_mapping.get(symptom, 0) for symptom in symptom_list]\n",
    "\n",
    "\n",
    "def compute_relatedness(code1, code2):\n",
    "    \"\"\"\n",
    "    Computes relatedness between two ICD-11 codes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if either code is NaN or not a string\n",
    "    if not isinstance(code1, str) or not isinstance(code2, str):\n",
    "        return 0\n",
    "    \n",
    "    # Check if codes are exactly the same\n",
    "    if code1 == code2:\n",
    "        return 1.0\n",
    "    \n",
    "    # Handle custom codes\n",
    "    custom_prefixes = [\"AAAA\", \"BBBB\", \"CCCC\"]\n",
    "    if code1[:4] in custom_prefixes or code2[:4] in custom_prefixes:\n",
    "        if code1[:4] == code2[:4]:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Check prefix relatedness for genuine ICD-11 codes\n",
    "    common_prefix_length = 0\n",
    "    min_length = min(len(code1), len(code2))\n",
    "\n",
    "    for i in range(min_length):\n",
    "        if code1[i] == code2[i]:\n",
    "            common_prefix_length += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Add additional weight if first four characters are same\n",
    "    if code1[:4] == code2[:4]:\n",
    "        common_prefix_length += 1\n",
    "        denominator = max(len(code1), len(code2)) + 1\n",
    "    else:\n",
    "        denominator = max(len(code1), len(code2))\n",
    "\n",
    "    return common_prefix_length / denominator\n",
    "\n",
    "\n",
    "def compute_relatedness_matrix(symptoms_list):\n",
    "    matrix = []\n",
    "    for symptom1 in symptoms_list:\n",
    "        row = []\n",
    "        for symptom2 in symptoms_list:\n",
    "            row.append(compute_relatedness(symptom1, symptom2))\n",
    "        matrix.append(row)\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare the data: Create a list of symptom sequences for each disease\n",
    "\n",
    "symptom_cols = [f\"Symptom_{i}\" for i in range(1, 26)]\n",
    "symptoms_data = df[symptom_cols].values.tolist()\n",
    "symptoms_data = [list(filter(lambda v: v==v, lst)) for lst in symptoms_data]\n",
    "\n",
    "\n",
    "df['relatedness_values'] = df[symptom_cols].apply(compute_relatedness_matrix, axis=1)\n",
    "df['severity_values'] = df[symptom_cols].apply(get_severity_scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Step 4: Convert symptom sequences to embeddings using Word2Vec and process severity metrics\n",
    "\n",
    "embedding_size = 128\n",
    "w2v_model = Word2Vec(sentences=symptoms_data, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "def symptoms_to_embedding(symptom_codes, w2v_model, embedding_size, severity_scores):\n",
    "    # Convert symptom codes to embeddings\n",
    "    symptom_embeddings = [w2v_model.wv[code] if code in w2v_model.wv else np.zeros(embedding_size) for code in symptom_codes]\n",
    "    \n",
    "    # Concatenate embeddings with severity scores\n",
    "    combined_features = [np.concatenate([embed, [severity_scores[i]]]) for i, embed in enumerate(symptom_embeddings)]\n",
    "    \n",
    "    # Flatten the combined features\n",
    "    flattened_features = np.hstack(combined_features)\n",
    "    \n",
    "    return flattened_features\n",
    "\n",
    "X_embedded = np.array([symptoms_to_embedding(symptoms, w2v_model, embedding_size, severity) for symptoms, severity in zip(symptoms_data, df['severity_values'].tolist())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Prepare the data for training by combining the features\n",
    "\n",
    "max_dim = max([m.shape[0] for m in df['relatedness_values'].tolist()])\n",
    "relatedness_features = np.array([m.flatten() for m in df['relatedness_values'].tolist()])\n",
    "\n",
    "X_padded = pad_sequences(X_embedded, padding='post')\n",
    "X_padded_reshaped = X_padded.reshape(4802, -1)  # -1 makes numpy automatically compute the correct size for that dimension.\n",
    "# print(X_padded_reshaped.shape)  # This should print (4802, 3096)\n",
    "\n",
    "# print(X_padded.shape)\n",
    "# print(relatedness_features.shape)\n",
    "\n",
    "X_combined = np.hstack((X_padded_reshaped, relatedness_features))\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df[\"ICD 11\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define and compile the neural network model\n",
    "\n",
    "input_shape = (X_combined.shape[1],)\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=input_shape))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(y_encoded)), activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Split data and train the model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42)\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, y_train_onehot, validation_data=(X_test, y_test_onehot), epochs=epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Evaluate the model and visualize the training progress\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test_onehot)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_diseases_with_confidence(symptom_codes, top_n=5):\n",
    "    embedded_symptoms = symptoms_to_embedding(symptom_codes, w2v_model, embedding_size, get_severity_scores(symptom_codes))\n",
    "    padded_symptoms = pad_sequences([embedded_symptoms], maxlen=X_combined.shape[1], padding='post')\n",
    "    \n",
    "    # Reshape the padded symptoms to match the expected shape\n",
    "    reshaped_symptoms = padded_symptoms[:, :3721]\n",
    "    \n",
    "    # Convert to float32\n",
    "    reshaped_symptoms = reshaped_symptoms.astype(np.float32)\n",
    "    \n",
    "    prediction = model.predict(reshaped_symptoms)\n",
    "\n",
    "    top_n_indices = np.argsort(prediction[0])[-top_n:][::-1]\n",
    "    top_n_probabilities = prediction[0][top_n_indices]\n",
    "    top_n_icd_codes = label_encoder.inverse_transform(top_n_indices)\n",
    "    results = list(zip(top_n_icd_codes, top_n_probabilities))\n",
    "    return results\n",
    "\n",
    "symptom_input = ['MC15', '9D9Z', '9D90.6', '9C80.0', 'LD20.4', '8A68.Z', '9B73.3', '9B65.2', '1D01.Y', 'MA01.Z'] # 1F57.Z\tToxoplasmosis\n",
    "predictions_with_confidence = predict_top_diseases_with_confidence(symptom_input)\n",
    "for icd, confidence in predictions_with_confidence:\n",
    "    print(f\"Predicted disease (ICD 11 Code): {icd} with confidence: {confidence:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
