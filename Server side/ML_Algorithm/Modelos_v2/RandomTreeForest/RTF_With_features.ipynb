{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries and load the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(\"/home/guilherme/Documents/GitHub/Tese/MDCompass/\")\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from ML_Algorithm.Functions.compute_symptom_relatedness import compute_relatedness, compute_relatedness_matrix\n",
    "# from ML_Algorithm.Functions.compute_severity import get_severity_scores\n",
    "\n",
    "# Loading the severity scores\n",
    "with open(\"/home/guilherme/Documents/GitHub/Tese/MDCompass/ML_Algorithm/json_files/severity_scores.json\", \"r\") as f:\n",
    "    severity_mapping = json.load(f)\n",
    "\n",
    "# 1. Loading the Data\n",
    "file_path = \"/home/guilherme/Documents/GitHub/Tese/Documentation/Dataset_Augmentation/Augmented_database_29_09_2023.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Augmented Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Transformation and Encoding\n",
    "\n",
    "# Replace NaN with 'None' placeholder\n",
    "symptom_cols = [f'Symptom_{i}' for i in range(1, 26)]\n",
    "df[symptom_cols] = df[symptom_cols].fillna('None')\n",
    "df['symptoms'] = df[symptom_cols].apply(lambda row: [symptom for symptom in row if symptom != 'None'], axis=1)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "X = mlb.fit_transform(df['symptoms'])\n",
    "\n",
    "# Insert the additional features calculation here\n",
    "def get_severity_scores(symptom_list):\n",
    "    return [severity_mapping.get(symptom, 0) for symptom in symptom_list]\n",
    "\n",
    "\n",
    "# Calculating additional features\n",
    "df['relatedness_values'] = df['symptoms'].apply(compute_relatedness_matrix)\n",
    "df['severity_values'] = df['symptoms'].apply(get_severity_scores)\n",
    "\n",
    "# Padding the relatedness matrix to a consistent shape\n",
    "max_dim = max([len(x) for x in df['relatedness_values']])\n",
    "\n",
    "def pad_matrix_to_shape(matrix, max_dim):\n",
    "    # Determine how much to pad\n",
    "    pad_dim = max_dim - matrix.shape[0]\n",
    "    \n",
    "    # Pad rows\n",
    "    matrix = np.pad(matrix, ((0, pad_dim), (0, pad_dim)), 'constant')\n",
    "    return matrix\n",
    "\n",
    "# After calculating max_dim in the training phase\n",
    "with open('max_dim.json', 'w') as f:\n",
    "    json.dump({\"max_dim\": max_dim}, f)\n",
    "\n",
    "\n",
    "# Apply padding to each matrix\n",
    "df['relatedness_values_padded'] = df['relatedness_values'].apply(lambda x: pad_matrix_to_shape(x, max_dim))\n",
    "\n",
    "# Padding the severity values to have the same length\n",
    "padded_severity = pad_sequences(df['severity_values'], padding='post', dtype='float32', maxlen=max_dim)\n",
    "\n",
    "# Now, reshaping the relatedness and severity features\n",
    "relatedness_features = np.array(df['relatedness_values_padded'].tolist()).reshape(len(df), -1)\n",
    "\n",
    "severity_features = padded_severity.reshape(len(df), -1)\n",
    "\n",
    "# Stacking them up with the binarized symptoms\n",
    "# X = np.hstack((X, relatedness_features, severity_features))\n",
    "X = np.hstack((X, relatedness_features))\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['ICD 11'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the Data into Training and Testing Sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Introducing SMOTE\n",
    "smote = SMOTE(k_neighbors=4, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Fine-tuning\n",
    "# This is a simple example using GridSearchCV for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf_template = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "# Set up parameter grid (adjust parameters and ranges accordingly)\n",
    "param_grid = {\n",
    "    'n_estimators': [25, 50, 75, 100], \n",
    "    'max_depth': [None], \n",
    "    'min_samples_split': [2, 5, 10], \n",
    "    'max_features': [2, 5, 10, 15],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "grid_search = GridSearchCV(clf_template, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get best parameters and estimator\n",
    "best_params = grid_search.best_params_\n",
    "# best_clf = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train the Random Forest Model\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    random_state=42,\n",
    "    max_features= best_params['max_features'],\n",
    "    class_weight=best_params['class_weight'],\n",
    "    bootstrap=best_params['bootstrap']\n",
    ")\n",
    "clf.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Model Evaluation\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred, labels=range(len(label_encoder.classes_)), target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\",\n",
    "            cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Model Deployment\n",
    "import joblib\n",
    "\n",
    "# Save the model, label encoder, and binarizer for later use\n",
    "model_filename = 'RTF_with_features.pkl'\n",
    "label_encoder_filename = 'with_features_label_encoder.pkl'\n",
    "binarizer_filename = 'with_features_symptoms_binarizer.pkl'\n",
    "\n",
    "joblib.dump(clf, model_filename)\n",
    "joblib.dump(label_encoder, label_encoder_filename)\n",
    "joblib.dump(mlb, binarizer_filename)\n",
    "\n",
    "loaded_model = joblib.load(model_filename)\n",
    "loaded_label_encoder = joblib.load(label_encoder_filename)\n",
    "loaded_binarizer = joblib.load(binarizer_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Predict the disease\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def predict_disease(symptoms_list, top_n=5):\n",
    "    \"\"\"\n",
    "    Given a list of symptoms (ICD-11 codes), predict the top potential diseases along with their confidence.\n",
    "    \"\"\"\n",
    "    # Transform the symptoms list into the appropriate binary vector format\n",
    "    symptoms_encoded = loaded_binarizer.transform([symptoms_list])\n",
    "\n",
    "    # Calculate the relatedness and severity features\n",
    "    relatedness_values = compute_relatedness_matrix(symptoms_list)\n",
    "    severity_values = get_severity_scores(symptoms_list)\n",
    "    \n",
    "    # Flatten the relatedness values matrix\n",
    "    flattened_relatedness = relatedness_values.reshape(-1)\n",
    "    \n",
    "    # Pad the flattened relatedness values to the length of max_dim^2\n",
    "    padded_relatedness = pad_sequences([flattened_relatedness], maxlen=max_dim*max_dim, padding='post', dtype='float32').reshape(1, -1)\n",
    "\n",
    "    padded_severity = pad_sequences([severity_values], maxlen=max_dim, padding='post', dtype='float32').reshape(1, -1)\n",
    "\n",
    "    # Combine all the features\n",
    "    combined_features = np.hstack((symptoms_encoded, padded_relatedness, padded_severity))\n",
    "    # combined_features = np.hstack((symptoms_encoded, padded_relatedness))\n",
    "\n",
    "    # Predict the probability distribution over classes using the trained model\n",
    "    disease_probabilities = loaded_model.predict_proba(combined_features)\n",
    "\n",
    "    # Get indices of the top_n classes\n",
    "    top_indices = np.argsort(disease_probabilities[0])[-top_n:][::-1]\n",
    "\n",
    "    # Decode these indices to get the actual disease codes\n",
    "    top_diseases = loaded_label_encoder.inverse_transform(top_indices)\n",
    "    \n",
    "    # Extract their corresponding probabilities\n",
    "    top_probabilities = disease_probabilities[0][top_indices]\n",
    "\n",
    "    return list(zip(top_diseases, top_probabilities))\n",
    "\n",
    "# Test the prediction function\n",
    "sample_symptoms = ['MC15', '9D9Z', '9D90.6', '9C80.0', 'LD20.4', '8A68.Z', '9B73.3', '9B65.2', '1D01.Y', 'MA01.Z'] # 1F57.Z\tToxoplasmosis\n",
    "predicted_diseases_with_confidence = predict_disease(sample_symptoms)\n",
    "\n",
    "for disease, confidence in predicted_diseases_with_confidence:\n",
    "    print(f\"Disease: {disease} with confidence: {confidence*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
